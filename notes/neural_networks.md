**See backprop_notes : 1 to 4 images**

**Activation function** : A mathematical function applied to the output of a neuron. It introduces non-linearity into the model.

Examples : sigmoid - slow learner ; relu - fast learner; softmax

![](..\img\image.png)

**Gradient Descent** : 

Tries to move down a gradient (steepest direction) and reach a local minimum (easier than looking for global minimum). That is, compute gradient , take a small step in negative direction of gradient and continue.

Optimizers : SGD , Adam (more common with neural networks), etc

**Classification vs Regression**

The main difference between Regression and Classification algorithms that Regression algorithms are used to predict the continuous values such as price, salary, age, etc. and Classification algorithms are used to predict/Classify the discrete values such as Male or Female, True or False, Spam or Not Spam, etc.